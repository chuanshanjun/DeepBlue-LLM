依赖信息:

书本简介：
第1课 机器学习快速上手路径--唯有实战  
1.1 机器学习的家族谱  
1.1.2 机器学习就是从数据中发现规律  
什么是机器学习：机器学习关键内涵之一在于计算机的运算能力从大量的数据中发现一个“函数”或“模型”，并通过它来模拟现实世界事物间的关系。

机器学习的另外一个特质是从错误中学习，这一点也与人类的学习方式非常相似

1.1.3 机器学习的类别--监督学习及其他  
监督学习(supervised learning)、无监督学习(unsupervised learning)、半监督学习(semi-supervised learning)。
监督学习的训练需要标签数据，而无监督学习不需要标签数据，半监督学习介于两者之间。

1.1.4 机器学习的重要分支--深度学习  
深度学习采用**神经网络(Artificial Neural Network, ANN)**是数据结构和算法形成的机器学习模型，由大量的所谓人工神经元相互联结而成，这些神经元
都具有可以调整的参数，可以实现监督学习或者无监督学习。

初期的神经网络模型比较简单，后来人们发现网络层数越多，效果越好，就把**层数较多、结构比较复杂
的神经网络机器学习技术叫做深度学习**，神经网络本质上与其他机器学习方法一样，也是统计学方法
的一种应用，只是它结构更深、参数更多。

1.1.6 机器学习的两大应用场景--回归与分类
* 回归(regression): **回归问题**通常用来预测一个值，其**标签**的值是**连续**的。
* 分类(classification): **分类问题**是将事物标记一个类别**标签**，结果为**离散**值，
也就是类别中的一个选项。

1.2 快捷的云实战学习模式  
1.2.3 用Google Colab开发第一个机器学习程序  
Sklearn线性回归模型的score属性给出的是R2分数，它是一个 ，给出的是预测值的方差与总体方 差之间的差异。

1.5 机器学习项目实战架构  
1.5.1 第一个环节：问题定义  
**重要**:在机器学习中，不是一开始就建立模型，而是首先构建你的问题。反复问一问自己、问一问客户
和其他项目干系人，目前的痛点是什么、要解决的问题是什么、目标是什么。--可是在现实中最关键的问题最
可能被忽略。

**如何评判机器学习是否生效**:如果机器学 习无法预测历史，它就无法预测未来。这是因为机器学习只能
识别出 它曾经见过的东西。要想在过去的数据的基础上预测未来，其实存在 一个假设，就是未来的规律与过去相同。


第2课 数学和Python基础知识  
2.1 函数描述了事物间的关系  
2.1.2 机器学习中的函数  
机器学习的目的是进行 预测、判断，实现某种功能。通过学习训练集中的数据，计算机得到 一个从x到y
的拟合结果，也就是函数。然后通过这个函数，计算机就能够从任意的x，推知任意的y。  

无论是传统的机器学习，还是深度学习，所得到的函数模型都是 对样本集中特征到标签的关系的总结，
是其相关性的一种函数化的表达。

2.2 捕捉函数的变化趋势  
2.2.3 凸函数有一个全局最低点  
因为在机器学习的梯度下降过程中，只有凸函数能够确保下降到全局最低点。  

2.3 梯度下降是机器学习的动力之源  
2.3.1 什么是梯度  
* 梯度：两个自变量的函数f(x1，x2)，对应着机器学习数据集中的两个特征，如果分别对x1，x2求偏导数
那么求得的梯度**向量**就是(∂ /∂ 1，∂ /∂ 2)T，在数学上可以表示成Δf( 1， 2)  
* 梯度几何意义：就是函数变化的 方向，而且是变化最快的方向。

2.3.2 梯度下降：下山的隐喻

2.3.3 梯度下降有什么用  
* 机器学习的本质是找到最优的函数
* 如何衡量函数是否最优?其方法是尽量减小预测值和真值间的误差(在机器学习中也叫损失值)
* 可以建立误差和模型参数之间的函数(最好是凸函数)
* 梯度下降能够引导我们走到凸函数的全局最低点，也就是找到误差最小时的参数

2.4 机器学习的数据结构-张量  
2.4.3 向量--1D(阶)张量  
初学者在进行机器学习程序调试过程中，要坚持不懈地输出检查向量的维度，
以及张量的形状。因为一旦维度或张量形状出错了，机器学习建模过程是难以继续的......切记!

注意向量点积的结果是一个值，也就是一个标量，而不是一个向量。

2.4.4 矩阵--2D(阶)张量

2.4.5 序列数据--3D(阶)张量  
机器学习中统一把灰度图像和彩色图像视为4D张量  
序列数据集才是机器学习中的3D张量。而(time series)(简称时序)是最为常见的序列数据集

![时间序列数据集的张量结构](images/02/时间序列数据集的张量结构.png)  

2.4.6 图像数据--4D(阶)张量  
4D张量其形状为(样本，图像高度，图像宽度，颜色深度)  

![图像数据集的张量结构](images/02/图像数据集的张量结构.png)  

在机器学习中，不是对上万个数据样本同时进行处理，那样的话机器也受不了，而是一批一批地并行处理，
比如指定批量大小为64。此时每批的100px×100px的彩色图像张量形状为(64， 100，100， 3)，
如果是灰度图像，则为(64，100，100，1)。  

2.4.7 视频数据--5D(阶)张量  
视频可以看作是由一帧一帧的彩色图像组成的数据集。
* 每一帧都保存在一个形状为(高度，宽度，颜色深度)的3D张量中。
* 一系列帧则保存在一个形状为(帧，高度，宽度，颜色深度) 的4D张量中。  
因此，视频数据集需要5D张量才放得下，其形状为(样本，帧，高度，宽度，颜色深度)  


2.5 Python的张量计算     
2.5.1 机器学习中张量创建  
* Pyhton列表的元素在系统内存 中是分散存储的  
* NumPy 数组内 各元素则连续的存储在同一个内存块中，方便元素的遍历，
并可利用现代CPU的向量化计算进行整体并行操作，提升效率。因此NumPy数组要求
元素都具有相同的数据类型，而列表中各元素的类型则可以不同。
* 注意，直接赋值而得来的是Python内置的列表，要用**array方法**转换才能得到NumPy数组  
**注**：在Python中元组(tuple)其中元素不可修改，但list可以
* arange(a，b，c)函数产生a~b(不包括b)，间隔为c的一个数组;而linspace(a，b，c)函数是把a~b(包括b)，平均分成c份。

2.5.2 通过索引和切片访问张量数据  
* 索引(indexing)，就是访问整个数据集张量里面的某个 具体数据
* 切片(slicing)，就是访问一个范围内的数据。  
对多阶张量进行切片，只需要将不同轴上的切片操作用逗号隔开就好了。

2.5.3 张量的整体操作和逐元素运算  
* 张量的算术运算，包括加、减、乘、除、乘方等，既可以整体进行，也可以逐元素进行。
* 也可以对所有元素整体进行函数操作  

2.5.4 张量的变形和转置  
* 张量变形(reshaping)  
* 注意，调用reshape方法时，变形只是暂时的，调用结束后，张量本身并无改变。
如果要彻底地改变张量的形状需要重新赋值

2.5.5 Python中的广播

2.5.6 向量和矩阵的点积运算

2.6 机器学习的几何意义  
2.6.1 机器学习的向量空间  
机器学习模型是在更高维度的几何空间中对特征向量进行操作、变形，计算期间的距离，并寻找从特征向量
到标签之间的函数拟合-这就是从几何角度所阐述的机器学习本质。  

2.6.2 深度学习和数据流形  
深度学习的过程，实际上也就是一个数据提纯的过程。数据从比较粗放的格式，到逐渐变得“计算机友好”。  
数据为什么需要提纯呢?主要还是因为特征维度过高，导致特征空间十分复杂，进而导致机器学习建模过程难度过大。


第3课 线性回归--预测网店的销售额  
一般机器学习的步骤：  
1) 收集数据
2) 数据可视化
3) 特征工程(让数据更容易被机器处理)
4) 拆分数据-训练集、测试集
5) 特征缩放-把数据压缩到比较小的区间
6) 选择机器学习模型：确定机器学习算法、确定假设函数、确定损失函数
7) 通过梯度下降训练机器，确定模型内部参数的过程
8) 进行超参数调整和性能优化

3.2.5 数据集清洗和规范化
* 注意1: 对于回归问题的数值类型数据集，机器学习模型所读入的规范格式应该是2D张量
也就是矩阵，其形状为(样本数，标签数)
* 注意2: 在拆分数据前，要注意数据是否已经被排序或者分类，如果是
还需要先打乱。  

数据归一化公式：x=(x-min(x))/(max(x)-min(x))
* 注：归一化过程中的最大值、最小值，以及最大值和最小值的差，全部来自训练数据集。
**不能使用测试集中的数据信息进行特征缩放中间步骤中任何值计算**

3.3 选择机器学习模型  
3.3.3 损失(误差)函数--L(w,b)  
* 损失(loss): **是对糟糕预测的惩罚**，也就是**误差**，也称为**成本(cost)**或**代价**，
就是当前预测值和真实值之间的差距的体现。它是一个数值，表示对于单个样本而言模型预
测的准确程度。如果模型的预测完全准确，则损失为0；如果不准确，就有损失。
* 注: 针对每一组不同的参数，机器都会针对样本数据集算一次平均损失。
* 损失函数(loss function) L(w,b)就是用来计算平均损失的。也叫**代价函数、成本函数(cost function)**
* 注: 损失函数L是参数w和b的函数，不是针对x的函数。
* 注: **如果平均损失小，参数就好；如果平均损失大，模型或者参数就还要继续调整。**这个计算当前假设函数所造成的损失的过程，
就是前面提到过的**模型内部参数的评估**的过程。

**用于回归的损失函数**
1) 均方误差(Mean Square Error, MSE)函数，也叫平方孙树或L2损失函数
2) 平均绝对误差（Mean Absolute Error，MAE）函数，也叫L1损失函数。
3) 平均偏差误差（mean bias error）函数  

**用于分类的损失函数**
1) 交叉熵损失（cross-entropy loss）函数
2) 多分类SVM损失（hinge loss）函数

**均方误差函数**实现过程
1) 对于每一个样本，其预测值和真实值的差异为（y−y'），而y'=wx+b，所以损失值与参数w和b有关
2) 如果将损失值（y−y'）夸张一下，进行平方（平方之后原来有正 有负的数值就都变成正数），就变成（y−y'）2。我们把这个值叫作单个
样本的平方损失。
3) 需要把所有样本的平方损失都相加，写成求和则为下面的公式

![均方误差损失函数_01](images/03/均方误差损失函数_01.png)  
最后的公式为：
![均方误差损失函数_02](images/03/均方误差损失函数_02.png)  

4) 公式中的N前面还有常量2，是为了在求梯度的时候，抵消二次方后产生的系数，方便后续进行计算，同时增
加的这个常量并不影响梯度下降的最效结果。
5) L: **对于一个给定的训练样本集而言，它是权重w和偏置b的函数，它的大小随着w和b的变化而变。**
6) 使用MSE函数做损失函数的线性回归算法，有时被称为**最小二乘法**

3.4 通过梯度下降找到最佳参数  
3.4.1 训练机器要有正确的方向  

![梯度下降实现](images/03/梯度下降的实现.png) 

程序中用梯度下降法通过求导来计算损失曲线在起点处的梯度。此时，就是损失曲线导数的矢量，
它可以让我们了解哪个方向距离目 标“更近”或“更远”。  

* 如果求导后梯度为正值，则说明L正在随着w增大而增大，应该减小w，以得到更小的损失。
* 如果求导后梯度为负值，则说明L正在随着w增大而减小，应该增大w，以得到更小的损失。
* 注:此处在单个权重参数的情况下，损失相对于权重的梯度就称为导数;若考虑偏置，或存在多个权重参数时，
损失相对于单个权重的梯度 就称为偏导数。  

因此，通过对损失曲线进行求导之后，就得到了梯度。梯度具有以下两个特征。
1) 方向:(也就是梯度的正负)
2) 大小:(也就是切线倾斜的幅度)  
这两个重要的特征，尤其是方向特征确保了梯度始终指向损失函数中增长最为迅猛的方向。梯度下降法会沿着
负梯度的方向走一步，以降低损失，如下图:  

![梯度下降: 找到损失最小时的权重](images/03/梯度下降.png) 

通过梯度下降法，如果初始估计的值落在最优值左边，那么梯度下降会将w增大，以趋近最低值;
如果初始估计的w值落在最优值右边，那么梯度下降会将w减小，以趋近最低值。
这个逐渐趋近于最优值的过程也叫作损失函数的收敛。

![梯度公式](images/03/梯度公式.png) 

* 注: 上图有两个错误点：1）2N中的N应该消掉；2）应该是(y_hat - y) 不然的话前面要加负号

3.4.4 学习率也很重要  
* 学习速率(learning rate): 学习速率乘以损失曲线求导之后的微分值，就是一次梯度变化的步长(step size)
它控制着当前梯度下降的节奏，或快或慢, w将在每一次迭代过程中被更新、优化。

![权重更新公式](images/03/权重更新公式.png) 

* 超参数: 像学习速率、迭代次数这样的参数，我们称这类位于模型外部的人工可调节的参数为超参数。
* 模型参数: 而权重 、偏置，当然都是模型内部参数，由梯度下降负责优化，不需要人工调整。

学习率过大过小:

![学习率过大过小](images/03/学习率过大过小.png) 

寻找最佳学习率：一个常见的策略是，在机器学习刚刚开始的时候，学习速率可以设置得大一些，
快速几步达到靠近最佳权重的位置，当逐渐地接近最佳权重时，可以减小学习速率，防止一下子越过最优值。

3.5 实现一元线性回归模型并调试超参数  
3.5.1 权重和偏置的初始值    
3.5.2 进行梯度下降
3.5.3 调试学习速率  
3.5.5 在测试集上进行预测  
测试集损失比训练集损失还低，这种情形并不是机器学习的常态,但在比较小的数据集上是有可能出现的

3.6 实现多元线性回归模型  
多元(变量)的线性方程的假设函数:  

![多元假设函数](images/03/多元假设函数.png) 

其中： 

![wtx](images/03/wtx.png) 

* 注：如果w是一个向量，x也是一个向量，两个向量做乘法，会得到一个标量，也就是数值y'.
两个向量，你点积我，我点积你，结果是相同的。因此wT·x等于x·wT
* 为什么会有T:因为w和x这两个张量的实际形状为(N，1)的矩阵，它们直接相乘是不行的， 
其中一个需要先转置为(1，N)
1) 张量形状(1，N)点积(N，1)，就得到1×1的标量
2) 张量形状(N，1)点积(1，N)，那就得到(N，N)的矩阵,就不是我们想要的y'。
3) 张量形状(1，N)点积(1，N)，或者(N，1)点积(N，1)，就会出错

还可以把公式进一步简化，就是把b也看作权重w0，那么需要引入x0，这样公式就是:

![多元假设函数_02](images/03/多元假设函数_02.png) 

引入w0，就是给数据集添加一个新的哑(dummy)特征，值为1， 和这个哑特征相乘，值不变:

![多元假设函数_03](images/03/多元假设函数_03.png) 

3.6.2 多变量的损失函数和梯度下降  
损失函数也通过向量化来实现:
```python
def loss_function(X, y, W): # 这个是书上的实现
    y_hat = np.dot(X, W.T)
    y_hat = y_hat.reshape(-1, 1)
    loss = y_hat - y
    cost = np.sum(loss**2)/(2*len(X))
    return cost
```
```python
def loss_function(X, y, weight): # 但更喜欢我自己的，因为我先把weightD变成变成2D，同时还带有T的意味
    y_hat = np.dot(X, weight.reshape(-1,1)) # 点积运算h(x)=w0x0+w1x1+w2x2+w3x3
    loss = y_hat - y # 中间过程, 求 出当前W和真值的差值
    cost = np.sum((loss**2))/(2*len(X)) # 这是平方求和过程, 均方误差函数的代码实现
    return cost
```

权重更新的公式没有变化
![梯度下降公式](images/03/梯度下降的公式.png) 

3.6.4 初始化权重并训练机器


第4课 逻辑回归--给病患和鸢尾花分类  
4.1 问题定义：判断客户是否患病  
**机器学习的一大优势，就是可以对我们本身并不是特别理解的数据，也产生精准的洞见**  
4.2 从回归问题到分类问题  
4.2.1 机器学习中的分类问题  
机器学习的分类方法，找到一个合适额函数，拟合输入和输出关系，输入一个或一系列事物的特征
输出这个事物的类别  
输入特征_通过函数输出类别  

![输入特征_通过函数输出类别](images/04/输入特征_通过函数输出类别.png) 

* 注：机器不能够识别“男”，”女“，只能识别"1" "2"。这种文本到数值的转换是必做的特征工程  
而输出则是离散的数值，如0,1,2,3等分别对应不同的类别。  
这些类别之间是互斥关系，如一个动物是狗就不能是猫  
* 注：算法首先输出的其实是一个**可能性，可以把这个可能性理解成一个概率**
1) 机器学习模型根据输入判断一个人患心脏病的可能性为80%，那么就把这个人判定为”患病“类，输出数值1.
2) 机器学习模型根据输入判断一个人患心脏病的可能性为30%，那么判定这个人为”健康“，输出数值0
* 机器学习的分类过程，也就是确定某一事物隶属于某一个类型的**可能性大小**的过程

4.2.2 用线性回归+阶跃函数完成分类
* 注：对于分类编码为0、1的标签，一般分类阈值取0.5；如果分类编码为-1、+1，则分类阈值取0.  
通过这个阈值把回归的连续性结果转换成了分类的阶跃性、离散性的结果。

![0分离群样本](images/04/0分离群样本.png) 

这个同学考了0分不要紧，但是因为数据集的样本数量本来就不 多，一个离群的样本会造成线性回归
模型发生改变。为了减小平均误差，回归线现在要往0分那边稍作移动。因此，概率0.5这个阈值点所
对应的x分数也发生了移动，目前变成了50分。这样，如果有一个同学考了51分，本来是没有及格，
却被这个模型判断为及格（通过考试的概率高于0.5）。这个结果与我们的直觉不符。
* 注：放在这是想说明，离群样本对模型的影响，可能这就是要对特征数据做标准化，归一化的原因之一吧

4.2.3 通过Sigmoid函数进行转换  
因为这个函数对于靠近0分和100分附近的极端样本是很不敏感的，类似样本的分类概率将无限逼近0或1，
样本个数再多也无所谓。但是在0.5这个分类概率临界点附近的样本将对函数的形状产生较大的影响。
也就是说，样本越靠近分类阈值，函数对它们就越敏感。

![sigmoid_01](images/04/sigmoid_01.png) 

* 逻辑函数(logistic function)在机器学习中，logistic function被广泛应用于逻辑回归分类和
神经网络激活过程。  

![sigmoid_公式](images/04/sigmoid_公式.png) 

* 注:为什么这里自变量的符号用的是z而不是x？因为它是一个中间变量，代表的是线性回归的结果。
而这里g（z）输出的结果是一个0～1的数字，也代表着分类概率。
* 通过Sigmoid函数就能够比阶跃函数更好地把线性函数求出的数值，
转换为一个0～1的分类概率值。

* Sigmoid 代码实现
```python
y_hat = 1/(1+ np.exp(-z)) # 输入中间变量z, 返回y'
```

4.2.4 逻辑回归的假设函数  
1) 首先通过线性回归模型求出一个中间值z，z=w0x1+w1x1+…+wnxn+b=WTX。它是一个连续值， 
区间并不在［0，1］之间，可能小于0或者大于1，范围从无穷小到无穷大。
2) 然后通过逻辑函数把这个中间值z转化成0～1的概率值，以提高拟合效果$g(z) = \frac{1}{1 + e^{-x}}$
3) 结合步骤（1）和（2），把新的函数表示为假设函数的形式:
$$h(x) = \frac{1}{1 + e^{-(w^T x)}}$$
4) 最后还要根据y'所代表的概率，确定分类结果
* 如果h（x）值大于等于0.5，分类结果为1
* 如果h（x）值小于0.5，分类结果为0

![逻辑回归模型步骤](images/04/逻辑回归模型步骤.png) 

综上，逻辑回归所做的事情，就是把线性回归输出的任意值，通过数学上的转换，输出为0～1的结果，
以体现二元分类的概率（严格来说为后验概率）。

* 上述过程中的关键在于选择Sigmoid函数进行从线性回归到逻辑回归的转换。

Sigmoid函数的优点如下
1) Sigmoid函数是连续函数，具有单调递增性（类似于递增的线性函数）
2) Sigmoid函数具有可微性，可以进行微分，也可以进行求导
3) 输出范围为［0，1］，结果可以表示为概率的形式，为分类输出做准备。
4) 抑制分类的两边，对中间区域的细微变化敏感，这对分类结果拟合效果好。

4.2.5 逻辑回归的损失函数  
在逻辑回归中，不能使用MSE。因为经过了一个逻辑函数的转换之后，MSE对于w和b而言，不再是一个凸函数，
这样的话，就无法通过梯度下降找到全局最低点，如下图所示。

![mse对逻辑回归不再是凸函数](images/04/mse对逻辑回归不再是凸函数.png) 

为了避免陷入局部最低点，我们为逻辑回归选择了符合条件的新的损失函数，公式如下

![逻辑回归_损失函数](images/04/逻辑回归_损失函数.png) 

这是一个函数在真值为0或者1的时候的两种情况:

![逻辑回归_损失函数_图像](images/04/逻辑回归_损失函数_图像.png) 

* 如果真值是1，但假设函数预测概率接近于0的话，得到的损失值将是巨大的。
* 如果真值是0，但假设函数预测概率接近于1的话，同样将得到天价的损失值。

![逻辑回归_损失函数_公式](images/04/逻辑回归_损失函数_公式.png) 

这个公式其实等价于上面的损失函数在0、1时的两种情况，同学们可以自己代入y = 0和y = 1
两种取值分别推演一下。

代码实现:
```python
loss = - (y_train*np.log(y_hat) + (1-y_train)*np.log(1- y_hat))
```

4.2.6 逻辑回归的梯度下降
我们所选择的损失函数经过Sigmoid变换之后是可微的，也就是说每一个点都可以求导，
而且它是凸函数，存在全局最低点。梯度下降的目的就是把w和b调整、再调整，直至最低的损失点。  

逻辑回归的梯度下降过程和线性回归一样，也是先进行微分，然后把计算出来的导数乘以一个学习速率α，
通过不断的迭代，更新w和b，直至收敛。

逻辑回归的梯度计算公式如下：
![逻辑回归_梯度计算公式_01](images/04/逻辑回归_梯度计算公式_01.png) 

![逻辑回归_梯度计算公式_02](images/04/逻辑回归_梯度计算公式_02.png) 

![逻辑回归_梯度计算公式_03](images/04/逻辑回归_梯度计算公式_03.png) 

代码实现:
```python
def loss_function(X, y, w, b): # 注 为了与X进行矩阵点积操作，把W直接构建成2D矩阵
    y_hat = sigmoid(np.dot(X, w) + b) # Sigmoid逻辑函数 + 线性函数(wX +b) 得到y_hat 
    # 那么点积之后生成的y_hat，就是一个形状为（242，1）的张量，其中存储了每一个样本的预测值。
    loss = - (y*np.log(y_hat) + (1-y)*np.log(1 - y_hat)) # 计算损失
    #   语句loss = -((y*np.log(y_hat) + (1-y)*np.log(1-y_hat))计算了每一个
    # 样本的预测值y'到真值y的误差，其中用到了Python的广播功能，比如1-y
    # 中的标量1就被广播为形状（242，1）的张量。
    cost = np.sum(loss)/X.shape[0] # 整个数据集的平均损失
    # 语句cost = np.sum（loss） / X.shape［0］是将所有样本的误差
    # 取平均值，其中X.shape［0］就是样本个数，cost，英文意思是成本，也
    # 就是数据集中各样本的平均损失。
    return cost # 返回整个数据集的平均损失

# 逻辑回归的梯度下降过程
def gradient_descent(X, y, w, b, lr, iter): # 定义逻辑回归梯度下降
    l_history = np.zeros(iter)
    w_history = np.zeros((iter, w.shape[0], w.shape[1]))
    b_history = np.zeros(iter)
    for i in range(iter):
        y_hat = sigmoid(np.dot(X, w) + b) # sigmoid函数 + 线性函数(wX +b)得到y_hat
        loss = -(y*np.log(y_hat) + (1-y)*np.log(1-y_hat)) # 计算损失
        d_w = np.dot(X.T, (y_hat - y))/X.shape[0]
        d_b = np.sum(y_hat - y)/X.shape[0]
        w = w - lr*d_w
        b = b - lr*d_b
        l_history[i] = loss_function(X,y,w,b)
        print('轮次', i+1, '当前训练集损失: ', l_history[i])
        w_history[i] = w # 注意w_history和w的形状
        b_history[i] = b
    return l_history, w_history, b_history
```

4.3 通过逻辑回归解决二元分类问题
4.3.1 数据的准备与分析  
* 注：首先查看标签数据，因为如果某一类别比例特别低（例如300个数据中只有3个人患病），
那么这样的数据集直接通过逻辑回归的方法做分类可能是不适宜的。

* 注: 在有些数据集中，MinMaxScaler()进行的数据特征缩放不仅不会提高效率，
似乎还会令预测准确率下降。**这个结果提示我们：没有绝对正确的理论，实践才是检验真理的唯一标准。**

4.3.2 建立逻辑回归
* 注：# 还要注意权重的梯度是一个形状为（13，1）的张量，其维度和特征轴维度相同，
而偏置的梯度则是一个值。
* 注：w_history是一个3D张量，因为w已经是一个2D张量了，因此语句w_history［i］ = w，
就是把权重赋值给w_history的后两个轴。而w_history的第一个轴则是迭代次数轴。

4.3.3 开始训练机器
* 注：**一定要注意梯度方法里面的(y_hat - y) 如果是 （y - y_hat）则更新的时候 w = w - lr*d_w
,bias 更新也是**

* 注：**经过线性函数及Sigmoid函数出来的是一个概率，一定要根据阈值把它转换成你的目标，如0,1**

4.3.5 绘制损失曲线  

4.3.6 直接调用Sklearn
4.3.7 哑特征的使用  
* 如果原始数据，例如男、女这种字符，首先转换成0、1数据格式。
* heart数据集中的cp,thal,slope这样的数据类型，也代表列别，如cp(胸痛)，取值为0,1,2,3。这些分类
是与大小无关的。计算机会把它们理解为数值，认为3比2大，2比1大。这种把“胸痛类型”的类别
像“胸部大小”的尺码一样去解读是不科学的，会导致误判。因为这种类别值只是一个代号，它的意义和
年龄、身高这种连续数值的意义不同。解决的方法，是把这种**类别特征拆分成多个哑特征**，比如cp有0、
1、2、3这4类，就拆分成个4特征，cp_0为一个特征、cp_1为一个特征、cp_2为一个特征、cp_3为一个特征。
每一个特征都还原成二元分类，答案是Yes或者No，也就是数值1或0。
* 哑变量用以反映质的属性的一个人工变量，是量化了的质变量，通常取值为0或1。

4.4 问题定义：确定鸢尾花的种类
4.5 从二元分类到多元分类  
1) 二元分类的思路：：通过逻辑回归算法确定一个种类或者一种情况出现的概率。
2) 多元分类：就是多个类别，而且每一个类别和其他类别都是互斥的情况。

4.5.2 多元分类的损失函数  
多元分类的标签共有以下两种格式
* one-hot格式的分类编码，比如，数字0～9分类中的数字8，格式为［0，0，0， 0，0，0，0，1，0］。
* 直接转换为类别数字，如1、2、3、4。
* 如果通过one-hot分类编码输出标签，则应使用分类交叉熵（categorical crossentropy）作为损失函数。
* 如果输出的标签编码为类别数字，则应使用稀疏分类交叉熵（sparse categorical crossentropy）作为损失函数。

4.6 正则化、欠拟合和过拟合  
4.6.1 **正则化（regularization）**
* 规范化(normalization): 一般是把数据限定在需要的范围，比如[0，1],从而消除了数据量纲对建模的影响。
* 标准化(standardization): 化一般是指将数据正态分布，使平均值为0，标准差为1  
**它们都是针对数据做手脚，消除过大的数值差异，以及离群数据所带来的偏见。经过规范化和标准化的数据，
能加快训练速度，促进算法的收敛。”**
* 正则化: **正则化不是对数据的操作,机器学习中的正则化是在损失函数里面加惩罚项，
增加建模的模糊性，从而把捕捉到的趋势从局部细微趋势，调整到整体大概趋势。虽然一定程度上地放宽了建模
要求，但是能有效防止过拟合的问题，增加模型准确性。它影响的是模
型的权重。**

4.6.2 欠拟合和过拟合  
正则化技术所要解决的过拟合问题，连同欠拟合（underfit）一起，都是机器学习模型调优（找最佳模型）、
参数调试（找模型中的最佳参数）过程中的主要阻碍。

![3个机器学习模型对数据集的拟合](images/04/3个机器学习模型对数据集的拟合.png) 

项目初期会倾向于用比较简单的函数模型去拟合训练数据集，比如线性函数（上图第1个）。  
后来发现简单的函数模型不如复杂一点的模型拟合效果好，所以调整模型之后，有可能会得到
更小的均方误差（上图第2个）。  
如果继续追求更完美的效果，甚至接近于0的损失，可能会得到类似于上图第3个函数图形。

**模型好不好，不能单看训练集上的损失。或者说，不能主要看训练集上
的损失，更重要的是看测试集上的损失。**

![寻找模型优化和泛化的平衡点](images/04/寻找模型优化和泛化的平衡点.png) 

一开始模型“很烂”的时候，训练集和测试集的误差都
很大，这是**欠拟合**。随着模型的优化，训练集和测试集的误差都有所下
降，其中训练集的误差值要比测试集的低。这很好理解，因为函数是根
据训练集拟合的，泛化到测试集之后表现会稍弱一点。但是，如果此处
继续增加模型对训练集的拟合程度，会发现测试集的误差将逐渐升高。
这个过程就被称作**过拟合**。

* 注: 模型的复杂度可以代表迭代次数的增加（内部参数的
优化），也可以代表模型的优化（特征数量的增多、函数复杂度的提高，比如从线性函数到二次、
多次函数，或者说决策树的深度增加，等等）。
* 过拟合就是机器学习的模型过于依附于训练集的特征，因而
模型泛化能力降低的体现。**泛化**能力，就是模型从训练集移植到其他数
据集仍然能够成功预测的能力。
* 分类问题也会出现过拟合，如下图所示，过于细致的分类边界也造
成了过拟合。

![3个分类器的分类边界](images/04/3个分类器的分类边界.png) 

**过拟合现象是机器学习过程中怎么甩都甩不掉的阴影**，影响着模
型的泛化功能，因此我们几乎在每一次机器学习实战中都要和它作战！

降低过拟合现象通常有以下几种方法:
1) 增加数据集的数据个数。数据量太小时，非常容易过拟合，因为
小数据集很容易精确拟合。
2) 找到模型优化时的平衡点，比如，选择迭代次数，或者选择相对
简单的模型。
3) **正则化**为可能出现过拟合现象的模型增加正则项，通过降低模
型在训练集上的精度来提高其泛化能力，这是非常重要的机器学习思想
之一

4.6.3 正则化参数
机器学习中的正则化通过引入模型参数λ（lambda）来实现

![加入了正则后的损失函数](images/04/加入了正则后的损失函数.png) 

* 注：上图中的逻辑回归的损失函数应该是交叉熵损失函数

现在的训练优化算法是一个由两项内容组成的函数：一个是**损失
项**，用于衡量模型与数据的拟合度；另一个是**正则化项**，用于调解模型
的复杂度。

**正则化机制引入损失函数之后，当权重大的时候，损失被加大，λ值越大，惩罚越大。这
个公式引导着机器在进行拟合的时候不会随便增加权重。**

记住，正则化的目的是帮助我们减少过拟合的现象，而它的本质是
约束（限制）要优化的参数。  
其实，正则化的本质，就是**崇尚简单化**。同时以最小化损失和复杂
度为目标，这称为**结构风险最小化**。

**奥卡姆剃刀定律**认为科学家应该优先采用更简单的公式或
理论。将该理论应用于机器学习，就意味着越简单的模型，有可能具有
越强的泛化能力。

选择λ值的目标是在简单化和训练集数据拟合之间达到适当的平
衡。

* 如果λ值过大，则模型会非常简单，将面临数据欠拟合的风险。
此时模型无法从训练数据中获得足够的信息来做出有用的预测。而且λ
值越大，机器收敛越慢。
* 如果λ值过小，则模型会比较复杂，将面临数据过拟合的风险。
此时模型由于获得了过多训练数据特点方面的信息而无法泛化到新数
据。
* 将λ设为0可彻底取消正则化。在这种情况下，训练的唯一目的是
最小化损失，此时过拟合的风险较高。  
正则化参数通常有L1正则化和L2正则化两种选择。
* L1正则化，根据权重的绝对值的总和来惩罚权重。在依赖稀疏特
征（后面会讲什么是稀疏特征）的模型中，L1正则化有助于使不相关或
几乎不相关的特征的权重正好为0，从而将这些特征从模型中移除。
* L2正则化，根据权重的平方和来惩罚权重。L2 正则化有助于使
离群值（具有较大正值或较小负值）的权重接近于0，但又不会正好为
0。在线性模型中，L2 正则化比较常用，而且在任何情况下都能够起到
增强泛化能力的目的。

刚才给出的正则化公式实际上是L2正则化，因为权重w正则化时做了平方。

正则化不仅可以应用于逻辑回归模型，也可以应用于线性回归和其
他机器学习模型。应用L1正则化的回归又叫Lasso Regression（套索回
归），应用L2正则化的回归又叫Ridge Regression（岭回归）。  
而最佳λ值则取决于具体数据集，需要手动或自动进行调整。

4.7 通过逻辑回归解决多元分类问题  
4.7.1 数据的准备与分析    
4.7.2 通过Sklearn实现逻辑回归的多元分类  
```python
lr = LogisticRegression(penalty='l2', C = 0.1) # 设定L2正则化和C参数
```
* L2正则化，只是选择了正则化的参数类别，但是用多大的力度进行呢？C表示正则化的
力度，它与λ刚好成反比。C值越小，正则化的力度越大。

* Scikit-learn中的C参数与λ成反比关系，$\lambda = \frac{1}{C}$

4.7.3 正则化参数--C值的选择  
不同C值对分类精度的影响  

![C_01](images/04/C_01.png) 

![C_02](images/04/C_02.png) 

1) C取值越大，分类精度越大。注意，当C=1 000时图中左下方
的圆点，本来按照其特征空间的位置来说，应该被放弃纳入圆点类，但
是算法因为正则化的力度过小，过分追求训练集精度而将其划至山鸢尾
集（圆点类），导致算法在这里过拟合。
2) 当C值取值过小时，正则化的力度过大，为了追求泛化效
果，算法可能会失去区分度。

还可以绘制出测试精度随着C参数的不同取值而变化的学习曲线
（learning curve），这样，可以更清晰地看到C值是如何
影响训练集以及测试集的精度的。

如何选择C值  
1) 一个因素是应该观察比较高的测试集准确率
2) 另一个因素是训练集和测试集的准确率之差比较小，通常会
暗示更强的泛化能力。


第5课 深度神经网络-找出可能流失的客户  
5.1 问题定义：咖哥接手的金融项目   
5.2 神经网络的原理  
5.2.1 神经网络极简史  
5.2.2 传统机器学习算法的局限性  
* **结构化数据：** 预定义的数据结构
* **非结构化数据:** 没有什么预定义的数据结构，不方便用数据库存储，
比如，办公文档、文本、图片、网页、各种图像/音频/视频信息等，都是非结构化数据。
这些数据和人类的感觉、知觉相关。可称为**感知类数据**

5.2.3 神经网络的优势  
在神经网络中，重要的不是单个分道器，而是整个轨道网络中集体意见的组合结果。
**因此数据越多，投票者越多，就能获得越多的模式**。如果有数百万个投票者，就能获得
数十亿种模式。每一种模式都可以对应一种结果，都代表着一种极为具体的从输入到输出
的函数。这些不同的模式使网络拥有归类的能力。训练的数据越多，网络
就越了解一种模式属于哪一个类别，就能在未来遇到没有标签的图片时做
出更准确的分类。

因此，深度学习并不是去尝试定义到底什么是一只猫，而是通过大量
的数据和大量的投票器，把网络里面的开关训练成“猫通路”“狗通
路”。

深度学习的机理：它是用一串一串的函数，也就是层，堆叠起来，作用于输入数据，
进行**从原始数据到分类结果的过滤与提纯。**这些层通过权重来参数化，通过损失函数
来判断当前网络的效能，然后通过优化器来调整权重，寻找从输入到输出
的最佳函数。注意以下两点。
1) 学习：就是为神经网络的每个层中的每个神经元寻找最佳的权重。
2) 知识：就是学到的权重

5.3 从感知器到单隐层网络  
神经网络由神经元组成，最简单的神经网络只有一个神经元，叫感知器  
5.3.1 感知器是最基本的神经元  
5.4 用Keras单隐层网络预测客户流失率 
5.4.1 数据的准备与分析  
数据的清洗注意点  
* 性别：二元类别特征，可以将性别Male/Female 编码为 0/1
* 城市：多元类别特征，可以把多元类别特征转换为多个二元类别哑变量 

5.4.3 单隐层神经网络的Keras实现  
* 序贯(sequential)模型模型: 也叫**顺序模型**,是最常用的深度网络层和层间架构，也就是
一个层接着一个层，顺序地堆叠
* 密集(dense)层: 是最常用的深度网络层的类型，也称为**全连接层**，即当前层和下一层的所有
神经元之间全有连接

5.5 分类数据不平衡问题：只看准确率够嘛  
5.5.1 混淆矩阵、精确率、召回率、和F1分数 

* 准确率：

![准确率](images/05/accuracy.png) 

* 精确率：

![精确率](images/05/precision.png) 

* 召回率：

![召回率](images/05/recall.png) 

* F1分数：

![F1分数](images/05/F1.png) 

5.5.2 使用分类报告和混淆矩阵  
5.5.3 特征缩放的魔力 
* 数据标准化
$$ x' = \frac{x - \mu_x}{\sigma_x} $$

等价于:

$$ x' = \frac{x - mean(x)}{std(x)} $$

* 注：无论采用哪种方法，特征缩放的代码必须要放在数据集拆分之后
* 均值和标准差都是在训练数据上计算而得的，然后将**同样的均值和标准差应用于训练集和测试集**。

在机器学习中，原则上不能使用在测试数据上计算得到的任何结果训练机器或优化模型，造成的结果就是测
试数据信息泄露，尽管提高了测试集准确率，但影响了模型泛化效果。

5.5.4 阈值调整、欠采样和过采样

5.6 从单隐层神经网络到深度神经网络

5.6.1 梯度下降：正向传播和反向传播

5.6.2 深度神经网络中的一些可调参数
* 优化器
* 激活函数 
* 损失函数 
* 评估指标

5.6.3 梯度下降优化器

优化器与神经网络梯度下降有关

神经网络除了有局部最低点外还有鞍点，在这两个点上，导数都为零，导数没有方向了，不知道如何优化了

使用以下几点尽可能优化上面的情况
1) 神经网络权重参数随机初始化
2) 批量梯度下降 -> 提升计算效率
3) 随机梯度下降: 每次只随机选择一个样本来更新模型参数.因此，这种方法每轮次的学习
速度非常快，但是所需更新的轮次也特别多。随机梯度下降中参数更新的方向不如批量梯度下降精确，每次也并不
一定是向着最低点进行。但这种波动反而有一个好处：在有很多局部最低点的盆地区域中，随机地波动可能会
使得优化的方向从当前的局部最低点跳到另一个更好的局部最低点，最终收敛于一个较好的点，甚至是全局最
低点。
4) 小批量随机梯度下降
5) 动量SGD
6) 上坡时减少动量—NAG
7) 各参数的不同学习速率—Adagrad
8) 加权平均值计算二阶动量—RMSProp
9) 多种优化思路的集大成者—Adam
10) 涅斯捷罗夫Adam加速—Nadam

5.6.4 激活函数：从Sigmoid到ReLU
不过神经网络中间层的输出值，没有必要位于［0，1］区间，因为中间层只负责非线性激活，
并不负责输出分类概率和预测结果。
1) Sigmoid函数和梯度消失:
    * 梯度消失: 反向传播求误差时，需要对激活函数进行求导，将来自输出损失的反馈信号传播到更远的层。
    如果需要经过很多层，那么信号可能会变得非常微弱，甚至完全丢失，网络最终变得无法训练。
2) Tanh函数: 可以将连续实数映射到［-1，1］区间，Tanh函数是一个以0为中心的分布函数，它的速度比Sigmoid函数快，
然而并没有解决梯度消失问题。
    ![Tanh](images/05/Tanh.png) 
3) ReLU函数:能够解决梯度消失问题，Re LU函数的特点是单侧抑制，输入信号小于等于0时，输出
是0；输入信号大于0时，输出等于输入。Re LU对于随机梯度下降的收敛很迅速，因为相较于Sigmoid和
Tanh在求导时的指数运算，对ReLU求导几乎不存在任何计算量。
$$f(z) = max(0,z) $$
![relu](images/05/relu.png)
Re LU函数也有缺点，它训练的时候比较脆弱，容易“死掉”。而且不可逆，“死”了就“活”不过来了。比如，
一个非常大的梯度流过一个ReLU神经元，参数更新之后，这个神经元可能再也不会对任何输入进行激活反映。
所以用ReLU的时候，学习速率绝对不能设得太大，因为那样会“杀死”网络中的很多神经元。
4) LeakyReLU和PReLU
$$f(z) = max(εz,z) $$
![leaky_relu](images/05/leaky_relu.png)
其中ε是很小的负数梯度值，比如0.01。这样做的目的是使负轴信息不会全部丢失，
解决了ReLU神经元“死掉”的问题。因为它不会出现零斜率部分，而且它的训练速度更快。
但是Leaky Re LU有一个问题，就是在接收很大负值的情况下，LeakyRe LU会导致神经元饱和，
从而基本上处于非活动状态。
5) eLU函数
6) Sigmoid和Softmax函数用于分类输出
   * 二分类：Sigmoid函数
   * 多分类：Softmax函数，对于一个输入，做Softmax之后的输出的各种概率和为1。而当类别数等于2时，
Softmax回归就退化为Logistic回归，与Sigmoid函数的作用完全相同了。

5.6.5 损失函数的选择
1) 连续值向量的回归问题: 的均方误差损失函数
2) 二分类问题: 同样熟悉的二元交叉熵损失函数
3) 多分类问题: one-hot编码，则用分类交叉熵损失函数
4) 多分类问题，如果输出是整数数值，则使用稀疏分类交叉熵损失函数

5.6.6 评估指标的选择
1) 回归问题，MAE(平均绝对误差函数)
2) 分类问题，准确率，但是对于类别分布不平衡的情况，应辅以精确率、召回率、F1分数等
其他评估指标。 

5.7 用Keras预测客户流失  
5.7.1 构建深度神经网络
```python
ann = Sequential()
ann.add(Dense(units=12, input_dim=12, activation='relu')) # 添加输入层
ann.add(Dense(units=24, activation='relu')) # 添加隐层
ann.add(Dense(units=48, activation='relu'))
ann.add(Dense(units=96, activation='relu'))
ann.add(Dense(units=192, activation='relu'))
ann.add(Dense(units=1, activation='sigmoid')) # 添加输出层

# 编译神经网络, 指定优化器、损失函数, 以及评估指标
ann.compile(optimizer='rmsprop',  # 此处我们先试试RMSP优化器
            loss='binary_crossentropy', # 损失函数
            metrics=['acc']) # 评估指标
```

使用深度神经网络后，发现以下几个问题
1) 效率高：发现较深的神经网络训练效率要高于小型网络，一两个轮次之后，准确率迅速提升到0.84以上
2) F1分数下降：从准确率上看，没有什么提升；而从F1分数上看，目前这个比较深的神经网络反而不如简单
的单隐层神经网络，从0.58下降到0.55
3) 过拟合：从损失函数上看，因为随着轮次的增加，训练集的误差值逐渐减小，但是验证集的误差反而越来越大了。

5.7.2 换一换优化器  
rmsprop -> adam
1) F1略有上升
2) 仍然过拟合

5.7.3 神经网络正则化:添加Dropout层
从损失曲线上判断，对于小数据而言，深度神经网络由于参数数量太多，已经出现了过拟合的风险  

**Dropout定义**:

![dropout](images/05/dropout.png)

假设在训练过程中，某一层对特定数据样本输出一个中间向量。

* 使用Dropout之前，中间向量为［0.5，0.2，3.1，2，5.9，4］
* 使用Dropout之后，中间向量变为［0.5，0，0，2，5.9，0］
* Dropout比率:就是被设为0的输出特征所占的比例，通常为0.2～0.5。  
**注意，Dropout只是对训练集起作用，在测试时没有神经元被丢掉。**

* 加深网络同时辅以Dropout正则化的策略比用单隐层神经网络更好。

**其实准确率或者F1分数本身的提升并不重要，更有价值的是网络优化过程中
所做的各种尝试和背后的思路。**

5.8 深度神经网络的调试及性能优化  
关于深度神经网络的调试和性能优化，很多研究者认为并没有什么固
定规律去遵循。因此，除了一些基本的原则之外，不得不具体问题具体分析，不断地在实战中去培养直觉。  
下面介绍一些基本的思路。  
5.8.1 使用回调功能
1) Model Checkpoint:在训练过程中的不同时间点保存模型，也就是
保存当前网络的所有权重。
2) Early Stopping:如果验证损失不再改善，则中断训练。这个回调函
数常与Model Checkpoint结合使用，以保存最佳模型
3) Reduce LROn Plateau:在训练过程中动态调节某些参数值，比如优
化器的学习速率，从而跳出训练过程中的高原区
4) TensorBoard：将模型训练过程可视化

5.8.2 使用TensorBoard
略

5.8.3 神经网络中的过拟合
过拟合问题在所有机器学习模型（包括神经网络）中都是性能优化过程中最为关键的问题。

在损失函数图像上，当训练集上的损失越来越低，但是验证集（或测
试集）上的损失到了一个点后显著上升，或者振荡，这就表示出现了过拟
合的现象。

解决思路：
1) 首先，根据奥卡姆剃刀定律，在使用非常深的网络之前应三
思，因为网络越大，越容易过拟合。如果能够用较简单的小型网络解决问
题，就不要强迫自己使用大网络。
2) 一种思路是在训练大型网络之前使用少量数据训练一个较小的
模型，小模型的泛化好，再去训练更深、更大的网络。不然的话，费了很
多精力直接训练一个大网络，最后发现结果不好就白费力气了。
3) 另外，最常见且有效地降低神经网络过拟合的方法就是在全连
接层之间添加一些Dropout层。这是很好用的标准做法，不过Dropout层会
对训练速度稍有影响。
4) 最后，使用较低的学习速率配合神经元的权重正则化可能是解
决过拟合问题的手段之一。

5.8.4 梯度消失和梯度爆炸  
1) 梯度反向传播过程中的梯度消失：神经网络梯度下降的原理是将来自输出损失的反馈信号
反向传播到更底部的层。如果这个反馈信号的传播需要经过很多层，那么信号可能会变
得非常微弱，甚至完全丢失，梯度无法传到的层就好比没有经过训练一样。这就是梯度消失。
2) 梯度爆炸: 梯度爆炸则是指神经元权重过大时，网络中较前面层的梯度通过训
练变大，而后面层的梯度呈指数级增大。

**梯度爆炸和梯度消失问题都是因为网络太深、网络权重更新不
稳定造成的，本质上都是梯度反向传播中的连锁效应。**

解决方案:
1) 选择合适的激活函数
2) 权重正则化:这个方法不仅对过拟合有效，还能抑制梯度爆炸。

    Keras中的权重正则化:  
    1) L1正则化，加入神经元权重的绝对值作为惩罚项
    2) L2正则化，加入神经元权重的平方作为惩罚项。
    3) 同时加入L1和L2作为惩罚项

3) 批标准化（batch normalization）有时称为批归一化:意思就是将数据
标准化的思想应用于神经网络层的内部，使神经网络各层之间的中间特征
的输入也符合均值为0、标准差为1的正态分布。

在批标准化出现之前，解决过拟合和梯度消失问题的方法是在迭代过
程中调整学习速率，采取较小的学习速率，以及精细的初始化权重参数。

而批标准化使网络中间层的输入数据分布变得
均衡，因此可以得到更为稳定的网络训练效果，同时加速网络的收敛，减
少训练次数。

在Keras中，批标准化也是网络中一种特殊的层组件，通常放在全连
接层或者卷积层之后，对前一层的输入数据进行批量标准化，然后送入下
一层进行处理。

```python
from keras.layers.normalization import Batch Normalization #导入批标准化组件
ann.add(Dense(64, input_dim=14, init='uniform')) # 添加输入层
ann.add(Batch Normalization()) # 添加批标准化层
ann.add(Dense(64, init='uniform')) # 添加中间层
```

4) 残差连接:  
**真正解决梯度消失的“武器”是残差连接（residual connection）结
构。**  
基本思想是：在大型深度网络中（至少10层以上），让前面某层
的输出跨越多层直接输入至较靠后的层，形成神经网络中的捷径
（shortcut）。这样，就不必担心过大的网络中梯度逐渐消失的问题了。
残差连接结构在最新的深度神经网络结构中几乎都有出现，因为它对解决
梯度消失问题非常有效。


第6课 卷积神经网络--识别狗狗图像  
卷积神经网络：与普通神经网络的区别是它的**卷积层内的神经元只覆盖输入特征局部范围的单元**
，具有稀疏连接（sparse connectivity）和权重共享（weight shared）的特点，而
且其中的过滤器可以做到对图像关键特征的抽取。因为这一特点，卷积神经网络在图像识别
方面能够给出更好的结果。

6.5 用卷积网络给狗狗图像分类  
6.5.2 构建简单的卷积网络

第7课 循环神经网络--鉴定留言及探索系外行星  
卷积神经网络处理图像时虽然有一个通过滑动窗口抠取图块与卷积核进行卷
积操作的过程，但对于每张图像来说，仍然是一个整体操作。也就是
说，先处理左侧的特征图，还是先处理右侧的特征图，神经网络所得到
的结果是完全相同的，预测值与操作特征的次序无关。  
然而在面对语言文字的时候，特征之间的“次序”突然变得重要起
来。
循环神经网络，就是专门用于处理语言、文字、时序这类特征之间存在“次序”的问题。
这是一种循环的、带“记忆”功能的神经网络，这种网络针对序列性问题有其
优势。

Python深度学习(第2版)  
* RNN: 处理序列的方式是通过遍历所有元素，同时保存一个**状态**,其中包含与已查看内容相关的信息。  
实际上，**RNN**，是一种具有内部**环路(loop)**的神经网络。但SimpleRNN存在**梯度消失问题**
* LSTM单元作用: 允许过去的信息稍后重新进入，从而解决梯度消失问题 
* GRU(门控循环单元): 与LSTM非常类似，可以看作LSTM架构的精简版本
* 双向RNN: 双向RNN利用了RNN的顺序敏感性，它包含两个普通RNN(例如前面介绍过的GRU层和LSTM层),
每个RNN分别沿一个方向对输入序列进行处理(按时间正序和按时间逆序),然后将它们的表示合并在一起。
通过沿着两个方向处理序列，双向RNN能够捕捉到可能被单向RNN忽略的模式。

双向RNN非常适合用于文本数据或任何其他类型的数据，其中顺序很重要，但使用**哪种顺序**并不重要。

永远记住，所有交易本质上都是**信息套利**